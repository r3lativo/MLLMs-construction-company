{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_structure(structure_id):\n",
    "    # Where the rendered structures are\n",
    "    gold_processed_path = \"../data/structures/gold-processed\"\n",
    "\n",
    "    # Construct the path to the structure\n",
    "    structure_path = os.path.join(gold_processed_path, structure_id)\n",
    "\n",
    "    # Load the structure JSON\n",
    "    try:\n",
    "        json_path = os.path.join(structure_path, f\"{structure_id}.json\")\n",
    "        s_json = json.load(open(json_path, \"r\"))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Structure {structure_id} not found.\")\n",
    "    \n",
    "    s_images_list = []\n",
    "\n",
    "    # Load the images\n",
    "    for filename in os.listdir(structure_path):\n",
    "        # Check if the file has a JPG or JPEG extension (case insensitive)\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(structure_path, filename)\n",
    "            try:\n",
    "                # Open the image file using PIL\n",
    "                img = Image.open(img_path)\n",
    "                s_images_list.append(img)\n",
    "            except IOError:\n",
    "                raise IOError(f\"Warning: Could not open image {img_path}\")\n",
    "    \n",
    "    return s_json, s_images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, processor, conversation, role_name, images=None, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Generates a response for the given model using the (filtered) conversation history.\n",
    "    \n",
    "    - Filters out system messages that are not intended for the current model.\n",
    "    - role_name should be the current model's identifier (e.g., \"Architect\" or \"Builder\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter conversation for the current model.\n",
    "    filtered_conversation = filter_conversation(conversation, target_model=role_name)\n",
    "    \n",
    "    # Build the prompt using the processor's chat template.\n",
    "    prompt = processor.apply_chat_template(filtered_conversation, add_generation_prompt=True)\n",
    "\n",
    "    inputs = processor(\n",
    "        images=images,\n",
    "        text=prompt,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "\n",
    "    # Minimal cleanup to remove special tokens (adjust as needed)\n",
    "    #response_text = output.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n",
    "    return output[0].split(\"[/INST]\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "structure_id = \"C1_bell\"\n",
    "max_rounds = 10\n",
    "max_new_tokens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A, processor_A = initialize_model(model_id, device, 4)\n",
    "model_B, processor_B = initialize_model(model_id, device, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_json, s_images_list = load_structure(structure_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_round = 0\n",
    "conversation_history = setup_roles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "while current_round < 10:\n",
    "        print(f\"===== Round {current_round + 1} =====\")\n",
    "\n",
    "        # ----- Architect's Turn -----\n",
    "        # For the first turn, pass the images; later turns might not require images.\n",
    "        modelA_response = generate_response(\n",
    "            model=model_A,\n",
    "            processor=processor_A,\n",
    "            conversation=conversation_history,\n",
    "            role_name=\"Architect\",\n",
    "            images=s_images_list if current_round == 0 else None,\n",
    "            max_new_tokens=256\n",
    "        )\n",
    "\n",
    "        print(\"Architect: %s\", modelA_response)\n",
    "\n",
    "        # Append Architect's response to the conversation history.\n",
    "        conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": modelA_response}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # Check if Architect signaled to finish.\n",
    "        if \"[FINISH]\" in modelA_response:\n",
    "            pprint(\"Finishing conversation as indicated by Architect.\")\n",
    "            break\n",
    "\n",
    "        # ----- Builder's Turn -----\n",
    "        modelB_response = generate_response(\n",
    "            model=model_B,\n",
    "            processor=processor_B,\n",
    "            conversation=conversation_history,\n",
    "            role_name=\"Builder\",\n",
    "            images=None,\n",
    "            max_new_tokens=256\n",
    "        )\n",
    "\n",
    "        print(\"Builder: %s\", modelB_response)\n",
    "\n",
    "        # Append Builder's response to the conversation history.\n",
    "        conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": modelB_response}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        current_round += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm-con",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
