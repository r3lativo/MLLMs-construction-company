{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_structure(structure_id):\n",
    "    # Where the rendered structures are\n",
    "    gold_processed_path = \"../data/structures/gold-processed\"\n",
    "\n",
    "    # Construct the path to the structure\n",
    "    structure_path = os.path.join(gold_processed_path, structure_id)\n",
    "\n",
    "    # Load the structure JSON\n",
    "    try:\n",
    "        json_path = os.path.join(structure_path, f\"{structure_id}.json\")\n",
    "        s_json = json.load(open(json_path, \"r\"))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Structure {structure_id} not found.\")\n",
    "    \n",
    "    s_images_list = []\n",
    "\n",
    "    # Load the images\n",
    "    for filename in os.listdir(structure_path):\n",
    "        # Check if the file has a JPG or JPEG extension (case insensitive)\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(structure_path, filename)\n",
    "            try:\n",
    "                # Open the image file using PIL\n",
    "                img = Image.open(img_path)\n",
    "                s_images_list.append(img)\n",
    "            except IOError:\n",
    "                raise IOError(f\"Warning: Could not open image {img_path}\")\n",
    "    \n",
    "    return s_json, s_images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, processor, conversation, target_name, images=None, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Generates a response for the given model using the (filtered) conversation history.\n",
    "    \n",
    "    - Filters out system messages that are not intended for the current model.\n",
    "    - role_name should be the current model's identifier (e.g., \"Architect\" or \"Builder\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter conversation for the current model.\n",
    "    filtered_conversation = filter_conversation(conversation, target_model=target_name)\n",
    "    \n",
    "    # Build the prompt using the processor's chat template.\n",
    "    prompt = processor.apply_chat_template(filtered_conversation, add_generation_prompt=True)\n",
    "\n",
    "    inputs = processor(\n",
    "        images=images,\n",
    "        text=prompt,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "\n",
    "    # Minimal cleanup to remove special tokens (adjust as needed)\n",
    "    #response_text = output.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n",
    "    return output[0].split(\"[/INST]\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "structure_id = \"C1_bell\"\n",
    "max_rounds = 10\n",
    "max_new_tokens = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "model_A, processor_A = initialize_model(model_id, device, 4)\n",
    "model_B, processor_B = initialize_model(model_id, device, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_json, s_images_list = load_structure(structure_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_round = 0\n",
    "conversation_history = setup_roles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Round 1 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Alright, let's start building the target structure. From the top-down view, it looks like we have a base that is a 3x3 grid. We'll need to build up from there.\n",
      "\n",
      "For the base, we'll need to place blocks in the following pattern:\n",
      "- The top left corner should be purple.\n",
      "- The top right corner should be green.\n",
      "- The bottom left corner should be gray.\n",
      "- The bottom right corner should be red.\n",
      "\n",
      "Once you have the base set up, we can start building the structure. Let me know when you have the base set up, and we'll move on to the next step. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [BUILDER]  Understood. I will set up the base according to your instructions.\n",
      "\n",
      "add.(purple, 0, 0, 0)\n",
      "add.(green, 0, 0, 0)\n",
      "add.(gray, 0, 0, 0)\n",
      "add.(red, 0, 0, 0) \n",
      "===== Round 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ARCHITECT]  Great! Now that we have the base set up, let's move on to the next step.\n",
      "\n",
      "From the side view, we can see that there are two columns of blocks. The left column should be purple, and the right column should be green. The top of each column should be gray, and the bottom of each column should be red.\n",
      "\n",
      "Please let me know when you have completed this step, and we'll move on to the next one. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [BUILDER]  [BUILDER]  Understood. I will add the blocks according to your instructions.\n",
      "\n",
      "add.(purple, 0, 0, 1)\n",
      "add.(green, 0, 0, 1)\n",
      "add.(gray, 0, 0, 1)\n",
      "add.(red, 0, 0, 1)\n",
      "\n",
      "add.(purple, 0, 1, 1)\n",
      "add.(green, 0, 1, 1)\n",
      "add.(gray, 0, 1, 1)\n",
      "add.(red, 0, 1, 1)\n",
      "\n",
      "add.(purple, 0, 2, 1)\n",
      "add.(green, 0, 2, 1)\n",
      "add.(gray, 0, 2, 1)\n",
      "add.(red, 0, 2, 1)\n",
      "\n",
      "add.(purple, 0, 3, 1)\n",
      "add.(green, 0, 3, 1)\n",
      "add.(gray, 0, 3, 1)\n",
      "add.(red, 0, 3, 1) \n",
      "===== Round 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ARCHITECT]  [ARCHITECT]  Great job! Now that we have the base and columns set up, let's move on to the top of the structure.\n",
      "\n",
      "From the front view, we can see that there are two rows of blocks. The top row should be purple, and the bottom row should be green. The leftmost block in each row should be gray, and the rightmost block in each row should be red.\n",
      "\n",
      "Please let me know when you have completed this step, and we'll move on to the final step. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [BUILDER]  [BUILDER]  Understood. I will add the blocks according to your instructions.\n",
      "\n",
      "add.(purple, 0, 0, 2)\n",
      "add.(green, 0, 0, 2)\n",
      "add.(gray, 0, 0, 2)\n",
      "add.(red, 0, 0, 2)\n",
      "\n",
      "add.(purple, 0, 1, 2)\n",
      "add.(green, 0, 1, 2)\n",
      "add.(gray, 0, 1, 2)\n",
      "add.(red, 0, 1, 2)\n",
      "\n",
      "add.(purple, 0, 2, 2)\n",
      "add.(green, 0, 2, 2)\n",
      "add.(gray, 0, 2, 2)\n",
      "add.(red, 0, 2, 2)\n",
      "\n",
      "add.(purple, 0, 3, 2)\n",
      "add.(green, 0, 3, 2)\n",
      "add.(gray, 0, 3, 2)\n",
      "add.(red, 0, 3, 2) \n",
      "===== Round 4 =====\n",
      " [ARCHITECT]  [ARCHITECT]  [ARCHITECT]  Great job! You have successfully replicated the target structure.\n",
      "\n",
      "[FINISH] \n",
      "'Finishing conversation as indicated by Architect.'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "while current_round < 10:\n",
    "        print(f\"===== Round {current_round + 1} =====\")\n",
    "\n",
    "        # ----- Architect's Turn -----\n",
    "        # For the first turn, pass the images; later turns might not require images.\n",
    "        modelA_response = generate_response(\n",
    "            model=model_A,\n",
    "            processor=processor_A,\n",
    "            conversation=conversation_history,\n",
    "            target_name=\"Architect\",\n",
    "            images=s_images_list if current_round == 0 else None,\n",
    "            max_new_tokens=256\n",
    "        )\n",
    "        modelA_response = f\"[ARCHITECT] {modelA_response}\"\n",
    "        print(modelA_response)\n",
    "\n",
    "        # Append Architect's response to the conversation history.\n",
    "        conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": modelA_response}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        # Check if Architect signaled to finish.\n",
    "        if \"[FINISH]\" in modelA_response:\n",
    "            pprint(\"Finishing conversation as indicated by Architect.\")\n",
    "            break\n",
    "\n",
    "        # ----- Builder's Turn -----\n",
    "        modelB_response = generate_response(\n",
    "            model=model_B,\n",
    "            processor=processor_B,\n",
    "            conversation=conversation_history,\n",
    "            target_name=\"Builder\",\n",
    "            images=None,\n",
    "            max_new_tokens=256\n",
    "        )\n",
    "        modelB_response = f\"[BUILDER] {modelB_response}\"\n",
    "        print(modelB_response)\n",
    "\n",
    "        # Append Builder's response to the conversation history.\n",
    "        conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": modelB_response}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        current_round += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllm-con",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
