{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from judge_utils import load_all_results, main_path, results_path\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results into dataframe\n",
    "df_main = load_all_results(results_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder\n",
    "df_main = df_main.sort_values('run_time').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge columns for easier lookup\n",
    "df_main[\"merge_key\"] = df_main.apply(\n",
    "    lambda row: (\n",
    "        row[\"structure_id\"],\n",
    "        row[\"use_img\"],\n",
    "        row[\"use_json\"],\n",
    "        row[\"shot\"]\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "judge_files = {\n",
    "    \"BASE\": os.path.join(main_path, \"analysis\", \"judge_analysis_BASE.json\"),\n",
    "    \"CLAR_Q\": os.path.join(main_path, \"analysis\", \"judge_analysis_CLAR_Q.json\"),\n",
    "    \"COMM_SH_REF\": os.path.join(main_path, \"analysis\", \"judge_analysis_COMM_SH_REF.json\"),\n",
    "    \"IMPL_REF\": os.path.join(main_path, \"analysis\", \"judge_analysis_IMPL_REF.json\")\n",
    "}\n",
    "\n",
    "for label, filepath in judge_files.items():\n",
    "    try:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            judge_data = json.load(f)\n",
    "            \n",
    "        rating_dict = {}\n",
    "        for entry in judge_data:\n",
    "            key = (\n",
    "                entry[\"structure_id\"],\n",
    "                entry[\"use_img\"],\n",
    "                entry[\"use_json\"],\n",
    "                entry[\"shot\"]\n",
    "            )\n",
    "            rating_dict[key] = entry[\"rating\"]\n",
    "\n",
    "        # Map the new column from the dictionary, using the merged key\n",
    "        df_main[label] = df_main[\"merge_key\"].map(rating_dict)\n",
    "    except Exception:\n",
    "        #print(f\"{filepath} missing\")\n",
    "        continue  # or handle error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metrics from \"parsed_actions_with_metrics.json\"\n",
    "with open(os.path.join(main_path, \"analysis\", \"parsed_actions_with_metrics.json\"), \"r\") as f:\n",
    "    metrics_data = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary keyed by the composite key from metrics_data\n",
    "metrics_dict = {}\n",
    "for entry in metrics_data:\n",
    "    key = (\n",
    "        entry[\"structure_id\"],\n",
    "        entry[\"use_img\"],\n",
    "        entry[\"use_json\"],\n",
    "        entry[\"shot\"]\n",
    "    )\n",
    "    metrics_dict[key] = {\n",
    "        \"accuracy\": entry[\"accuracy\"],\n",
    "        \"precision\": entry[\"precision\"],\n",
    "        \"iou\": entry[\"iou\"],\n",
    "        \"action_format\": entry[\"action_format\"]\n",
    "    }\n",
    "\n",
    "# Map each metric onto df_main using the composite key column\n",
    "df_main[\"accuracy\"] = df_main[\"merge_key\"].map(\n",
    "    lambda key: metrics_dict[key][\"accuracy\"] if key in metrics_dict else None\n",
    ")\n",
    "df_main[\"precision\"] = df_main[\"merge_key\"].map(\n",
    "    lambda key: metrics_dict[key][\"precision\"] if key in metrics_dict else None\n",
    ")\n",
    "df_main[\"iou\"] = df_main[\"merge_key\"].map(\n",
    "    lambda key: metrics_dict[key][\"iou\"] if key in metrics_dict else None\n",
    ")\n",
    "df_main[\"action_format\"] = df_main[\"merge_key\"].map(\n",
    "    lambda key: metrics_dict[key][\"action_format\"] if key in metrics_dict else None\n",
    ")\n",
    "\n",
    "# Optionally drop the temporary composite key column if no longer needed:\n",
    "df_main.drop(columns=[\"merge_key\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df_main.drop(columns=[\"json_file\", \"Model\", \"Quantization\", \"Device\", \"Number of models\",\t\"Max new tokens\",\t\"Repetition Penalty\",\t\"Max rounds\", \"json_file\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[df_main[\"BASE\"] == 3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[(df_main[\"BASE\"] == 1) & (df_main[\"accuracy\"] > 0)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[df_main[\"accuracy\"] > 0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rounds analysis when the architect finishes the conversation\n",
    "df_main[df_main[\"finished_by_architect\"] == True].num_rounds.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main[(df_main[\"accuracy\"] > 0) & (df_main[\"precision\"] > 0)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just 1 over 6 good accuracy ratings the one-shot was used\n",
    "df_main[df_main[\"shot\"] == \"one-shot\"].accuracy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to numeric, coercing errors to NaN if necessary.\n",
    "df_main[\"BASE\"] = pd.to_numeric(df_main[\"BASE\"], errors=\"coerce\")\n",
    "df_main[\"accuracy\"] = pd.to_numeric(df_main[\"accuracy\"], errors=\"coerce\")\n",
    "df_main[\"precision\"] = pd.to_numeric(df_main[\"precision\"], errors=\"coerce\")\n",
    "df_main[\"iou\"] = pd.to_numeric(df_main[\"iou\"], errors=\"coerce\")\n",
    "\n",
    "# Then group and compute the mean\n",
    "table = df_main.groupby([\"shot\", \"use_img\", \"use_json\"])[[\"BASE\", \"accuracy\", \"precision\"]].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = table.to_latex(float_format=\"%.2f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllms-con",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
