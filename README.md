# MLLMs Construction Company ğŸ‘·  
**Investigating Multimodal LLMs' Communicative Skills in a Collaborative Building Task**

Repository for the related project of Grounded Language Processing, course offered at UniTn in 2024/25


â €â €â €â €â €â €â €â €â €â €â €â¢€â£€â£€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €     
â €â €â €â €â €â €â €â¢€â£ â£´â ¾â¢»â£¿â¡Ÿâ »â ¶â¢¦â£¤â£€â¡€â €â €â €â €â €â €â €â €â €â €     
â €â €â €â €â£€â£¤â ¾â ›â ‰â €â €â£¸â ›â£·â €â €â €â €â ‰â ™â »â ¶â£¦â£¤â£€â €â €â €â €â €     
â €â €â â ›â ‹â €â €â €â €â €â €â ›â €â ›â ‚â €â €â €â €â €â €â €â €â ˆâ ™â ›â ’â ‚â €â €     
â €â¢¸â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡â €     
â €â €â €â¢ â£¤â£¤â£¤â €â €â €â €â¢ â£¤â¡„â¢ â£¤â¡„â €â €â €â €â €â €â €â €â €â¡„â €â €â €     
â €â €â €â ˆâ ‰â ‰â ‰â €â €â €â €â ¸â ¿â ‡â ¸â ¿â ƒâ €â €â €â €â €â €â €â €â €â¡‡â €â €â €     
â €â €â €â €â €â €â €â €â €â €â €â¢°â£¶â¡†â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €     
â €â €â €â €â €â €â €â €â €â €â €â¢¸â£¿â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €     
â €â €â €â €â €â €â €â €â €â €â €â¢¸â£¿â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â ƒâ €â €â €     
â €â €â €â €â €â €â €â €â €â €â €â¢¸â£¿â¡‡â €â €â €â €â €â €â €â €â €â €â €â¡€â ƒâ¡€â €â €     
â €â €â €â €â €â €â €â €â €â €â €â¢¸â£¿â¡‡â €â €â €â €â €â €â €â €â €â €â ˜â ‹â ˆâ ›â €â €     
â €â €â €â €â €â €â €â €â €â €â£€â¢¸â£¿â¡‡â¢€â €â €â €â €â €â €â €â €â €â£¿â£¿â£¿â£¿â¡‡â €     
â €â €â €â €â €â €â €â¢€â£´â ¾â ‹â¢¸â£¿â¡‡â ˆâ ³â£¦â¡€â €â €â €â €â €â €â €â €â €â €â €â €     
â €â €â €â €â €â €â €â ˆâ â €â €â ˆâ ›â ƒâ €â €â €â ‰â €â €â €â €â €â €â €â €â €â €â €â €     



In this project, we implement a collaborative building task where two multimodal large language models (MLLMs) assume the roles of Architect and Builder. The goal is to study how different prompting techniques (text-only, images-only, and mixed modalities) affect the modelsâ€™ ability to communicate in a human-like manner during a shared building task.

## Overview

The project aims to answer questions such as:
- How effective are MLLMs at replicating human-like communication in a collaborative task?
- Can multimodal prompts improve dialogue quality and task success?
- How do different experimental setups (zero-shot vs. one-shot) compare in performance?

Our experiments are inspired by previous work on collaborative building tasks in Minecraft-like environments and expand the setting to a fully automated scenario where both agents are MLLMs. Detailed methods, experimental design, and results are discussed in the accompanying report.

## Repository Structure

```plaintext
.
â”œâ”€â”€ analysis
â”‚   â”œâ”€â”€ judge_analysis_BASE.json
â”‚   â”œâ”€â”€ parsed_actions.json
â”‚   â”œâ”€â”€ parsed_actions_with_metrics.json
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ judge_data
â”‚   â”‚   â”œâ”€â”€ system_examples.json
â”‚   â”‚   â””â”€â”€ system_prompts.json
â”‚   â”œâ”€â”€ minecraft_corpus
â”‚   â”‚   â”œâ”€â”€ data-3-30
â”‚   â”‚   â”œâ”€â”€ data-format.md
â”‚   â”‚   â””â”€â”€ one_shot
â”‚   â””â”€â”€ structures
â”‚       â”œâ”€â”€ configs-to-names.txt
â”‚       â”œâ”€â”€ gold-configurations
â”‚       â”œâ”€â”€ gold-processed
â”‚       â””â”€â”€ terrain.xml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ results
â”‚   â””â”€â”€ [multiple log and JSON files from experiments]
â””â”€â”€ src
    â”œâ”€â”€ actions_evaluation.py
    â”œâ”€â”€ data_analysis.ipynb
    â”œâ”€â”€ data_preprocessing.py
    â”œâ”€â”€ judge.py
    â”œâ”€â”€ judge_utils.py
    â”œâ”€â”€ main.py
    â”œâ”€â”€ parse_and_convert.py
    â”œâ”€â”€ render.py
    â”œâ”€â”€ render_utils.py
    â”œâ”€â”€ run_experiments.sh
    â”œâ”€â”€ utils.py
    â””â”€â”€ worldstate_decompile.py
```

- **analysis/**: Contains scripts and output files for evaluating model performance, including parsed actions and metrics.
- **data/**: Houses all necessary input data such as judge prompts, the Minecraft dialogue corpus, and configuration files for target structures.
- **results/**: Stores experiment outputs (JSON logs, evaluation scores, etc.) for different experimental conditions.
- **src/**: Contains the main codebase:
  - `main.py` is the entry point that orchestrates the experiments.
  - `data_preprocessing.py` and `parse_and_convert.py` handle data and log processing.
  - `judge.py` and `judge_utils.py` implement evaluation of dialogue human-likeness.
  - Other modules (e.g., `render.py`, `actions_evaluation.py`) support visualization and action evaluation.
  - `data_analysis.ipynb` provides an interactive environment for further analysis.

## Installation and Setup

**Clone the Repository**

   ```bash
   git clone https://github.com/r3lativo/MLLMs-construction-company.git
   cd MLLMs-construction-company
   ```

Ensure you have Python 3.10+ installed. Then install the required packages:

   ```bash
   pip install -r requirements.txt
   ```

## Usage


  Use the provided shell script to run experiments:
  
  ```bash
  bash src/run_experiments.sh
  ```

  Alternatively, you can execute the main Python script, for example with:
  
  ```bash
  python src/main.py --structure_id C1
  ```


  Open the Jupyter notebook `src/data_analysis.ipynb` to explore and visualize experiment results.

## Connection to the Report

This repository underpins the experimental setup detailed in our report:

**Experimental Design**  
  The code implements a collaborative building task where two LLaVA-based MLLMs interact under different input modalities (text-only, images-only, mixed) and learning conditions (zero-shot, one-shot).

**Evaluation Metrics**  
  The evaluation scripts in the `analysis` folder calculate metrics such as task success rate, accuracy, precision, and human-likeness scores.

**Results**  
  Experiment logs and JSON results stored in the `results` directory are analyzed both quantitatively and qualitatively.

